# 2.2 Logistic regression model interpretation & hypothesis testings
## Odds Ratio Interpretation

### CHD example
Variables that might be related to the chance of developing this disease?
```{image} /image/chd1.png
:alt: fishy
:class: bg-primary mb-1
:width: 500px
:align: center
```
---
Fitting Model:

```{math}
:label: CHD
log\frac{p}{1-p} &= -4.502 + 0.025 Ã— height + 0.023 Ã— cigs \\
```
```{admonition} Odds Ratio Model?
:class: dropdown

$$ \frac{p}{1-p} =  e^{-4.502 + 0.025 Ã— height + 0.023 Ã— cigs } $$

$$ = e^{4.502} Â· e^{0.025Â·height} Â· e^{0.023Â·cigs } $$

$$ =0.011 Ã— 1.026 height Ã— 1.023 cigs $$
```

```{admonition} Interpretation?
:class: dropdown
The estimated odds ratio of heart disease for a man against another man 1 inch shorter is 1.026. The estimated odds of chd increase by 2.3% with each additional cigarette smoked per day.
```

## Hypothesis Testing

### Test significance of predictors
> LR test, Wald test, Score test can be applied to test: 
>> H0 : C Î² = Î¾ vs. H1 : C Î² = Î¾ 

H0: Î² has no effect (i.e. predictor not significant)<br>
H1: Î² has effect (i.e. predictor is significant)

#### Likelihood ratio test ðŸŒŸ (Mostly Used)
Test statistic
```{math}
\lambda &= -2 {l(\widetilde{\beta}) - l\hat{(\beta)}} \\
&=  D_w - D_\omega  \stackrel{d}{\approx} \chi^2(s)
```
where $\widetilde{\beta}$ is MLE under H0, and $\hat{(\beta)}$ is MLE under H1 <br>
for $\Delta D$, H0 with dof $ s = dim(Î²_{â„¦-Ï‰})$
#### Wald test
Test statistic
$$W = \frac{(\hat{\beta} - \beta_0)^2}{\mathrm{Var}(\hat{\beta})}$$

#### Score test
Test statistic
$$S = \frac{\partial \ell(\boldsymbol{\theta})/\partial \beta}{\sqrt{\mathrm{Var}(\partial \ell(\boldsymbol{\theta})/\partial \beta)}} \Bigg\rvert_{\beta = \hat{\beta}}$$

---

#### LR test Continued
```{math}
:label: deviance
D(\boldsymbol{\beta}) &= -2\big[\log L(\boldsymbol{\beta}) - \log L(\boldsymbol{0})\big] \\
&= -2 [\sum_{i=1}^{n} y_i \ln \left( \frac{\hat{y}_i}{y_i} \right) + (1-y_i) \ln \left( \frac{1-\hat{y}_i}{1-y_i} \right)] \\
&= -2[\sum_{i=1}^n \left[ y_i\log\left({p_i}\right) + (1-y_i)\log\left({1-p_i}\right)\right]]
```
The log-likelihood function can be seen at {eq}`log-like`

```{admonition} Deviance vs Residual Deviance
In other examples of GLMs, the deviance is a measure of how well the model fit the data. But in the case where the response is binary, the deviance cannot be used for measuring goodness of fit, because the deviance is just a function of pi â€™s

However, Residual deviance is can be used for measuring goodness of fit. It is the difference between 2 deviance which represents the deviance that adding one predictor reduces.
```

##### Chi-square Test & Residual Deviance
CHD example 2 (from prac)

The following data were obtained from a study of coronary heart disease, where N is the total number of subjects in each group and Y is the number diagnosed with coronary heart disease. The factor CHOL refers to serum cholesterol in mg/100cc where: 
1=<200, 2=200âˆ’219, 3=220âˆ’259, 4=260+ while the factor BP refers to blood pressure in mm of mercury where: 
1=<127, 2=127âˆ’146, 3=147âˆ’166, 4=167+ 

```{image} /image/chd.png
:alt: fishy
:class: bg-primary mb-1
:width: 200px
:align: center
```
---

It is important for us to transform this data into the form that R recognises:

| Y  | N   | BP | CHOL |
|----|-----|----|------|
| 2  | 119 | 1  | 1    |
| 3  | 124 | 2  | 1    |
| 3  | 50  | 3  | 1    |
| 4  | 26  | 4  | 1    |
| 3  | 88  | 1  | 2    |
| 2  | 100 | 2  | 2    |
| 0  | 43  | 3  | 2    |
| 3  | 23  | 4  | 2    |
| 8  | 127 | 1  | 3    |
| 11 | 220 | 2  | 3    |
| 6  | 74  | 3  | 3    |
| 6  | 49  | 4  | 3    |
| 7  | 74  | 1  | 4    |
| 12 | 111 | 2  | 4    |
| 11 | 57  | 3  | 4    |
| 11 | 44  | 4  | 4    |

---

```{code}
Y <- c(2, 3, 3, 4, 3, 2, 0, 3, 8, 11, 6, 6, 7, 12, 11, 11)
N <- c(119, 124, 50, 26, 88, 100, 43, 23, 127, 220, 74, 49, 74, 111, 57, 44)
BP <- factor(rep(1:4, 4))
CHOL <- factor(rep(1:4, rep(4, 4)))
fit.1 <- glm(Y/N ~ 1, weights = N, family = "binomial")
fit.2 <- glm(Y/N ~ CHOL, weights = N, family = "binomial")
fit.4 <- glm(Y/N ~ CHOL + BP, weights = N, family = "binomial")
summary(fit.2)
```


```{code}

Call:
glm(formula = Y/N ~ CHOL, family = "binomial", weights = N)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.659  -1.020   0.001   1.127   2.367  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -3.2419     0.2943 -11.017  < 2e-16 ***
CHOL2        -0.1839     0.4644  -0.396   0.6920    
CHOL3         0.5914     0.3480   1.699   0.0893 .  
CHOL4         1.4543     0.3392   4.287 1.81e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 58.726  on 15  degrees of freedom
Residual deviance: 26.805  on 12  degrees of freedom
AIC: 85.909
```


---


```{code}
# model4 CHD ~ CHOL + BP
Analysis of Deviance Table

Model: binomial, link: logit

Response: Y/N

Terms added sequentially (first to last)


     Df Deviance Resid. Df Resid. Dev
NULL                    15     58.726
CHOL  3   31.921        12     26.805
BP    3   18.729         9      8.076
```
given that 1-pchisq(8.076, 9) = 0.5265058

```{admonition} What can we interpret?
:class: dropdown
model4 CHD ~ CHOL + BP. Small Resid.Dev <=> big p-value <=>  model adequate <=> no significant association between CHOL & BP.
```

```{admonition} What if the Resid. Dev is big?
:class: dropdown
Model inadequate <=> there is a significant association between CHOL & BP.
```

##### Test Significance of Predictor (Coding)

```{code}
# model2 CHD ~ CHOL
Analysis of Deviance Table

Model: binomial, link: logit

Response: Y/N

Terms added sequentially (first to last)


     Df Deviance Resid. Df Resid. Dev
NULL                    15     58.726
CHOL  3   31.921        12     26.805
```

```{admonition} What does Resid. Dev mean?
:class: dropdown
deviance(fit.1) =  58.72622  (deviance of the null model)<br>
deviance(fit.2) = 26.80498 (residual deviance of the current model & full model)
```

```{tip}
H0: There is no significant relationship between the predictors and the response <br>
Hence p < 0.05: reject H0. Conclude: predictors significant <br>
Hence p > 0.05: can't reject H0. Conclude: predictors NOT significant
```

```{code}
1-pchisq(58.726-26.805, 3) #5.437987e-07
```
```{admonition} Are the predictors significant?
:class: dropdown
YES
```
```{admonition} Reasoning?
:class: dropdown
p-value = 5.437987e-07 < 0.05, reject H0: not significant. Conclude: predictor significant.
```


---

### Test Model Adequacy (Code)
#### 1 Model
Is model 2 adequate?
```{tip}
H0: The model provides a goodfit (model IS adequate) <br>
Hence p < 0.05: reject H0. Conclude: model is NOT adequate<br>
Hence p > 0.05: can't reject H0. Conclude: model IS adequate
```

```{code}
1-pchisq(26.805, 12) #0.008242301
```
```{admonition} Is the model adequate?
:class: dropdown
NO
```
```{admonition} Reasoning?
:class: dropdown
p-value = 0.008242301 < 0.05, reject H0: model is adequate. Conclude: model is not adequate.
```
#### 2 Models
Which model is a better choice mod2 or mod4?

```{warning}
ANOVA can only be used when models are nested.
```
```{code}
anova(fit.2, fit.4, test = "Chi")
```
```{code}
Analysis of Deviance Table

Model 1: Y/N ~ CHOL
Model 2: Y/N ~ CHOL + BP
  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
1        12    26.8050                          
2         9     8.0762  3   18.729 0.0003111 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
```

> 0.0003111 = 1-pchisq(26.8050 - 8.0762, 12 - 9)

```{admonition} H0?
:class: dropdown
H0: smaller model is fits the data better
```
```{admonition} Which model is better?
:class: dropdown
0.0003111 < 0.05, reject H0: smaller model better. Conclude: bigger model better (i.e. mod4 fits the data better)
```

## Confidence Interval
### Confidence Interval for $\beta$
Reminder of our fomualr {eq}`CHD`:
```{math}
log\frac{p}{1-p} &= -4.502 + 0.025 Ã— height + 0.023 Ã— cigs \\
```
where p is the probability of getting coronary heart disease 

Since 0.025 is only an estimated value, we want to find the confidence interval to indicate this.

```{code}
Coefficients:
            Estimate     Std. Error  z value   Pr(>|z|)    
(Intercept)  -4.5016140  1.8418627   -2.4441   0.01452 
height       0.0252078   0.0263274   0.9575    0.33833 
cigs         10.0231274  0.0040402   5.7243    1.038e-08

```
As we can see from the coef table, the Std. Error represents standard error which we usually use to present CI. Additionally,  $z\-value = \frac{est.}{std.}$. z value follows approx normal distribution, but we don't really use it.

```{admonition} 95% CI of $\beta_1$ (height) ?
:class: dropdown
0.0252078 Â± 1.96 Â· 0.0263274 = ( 0.0263939, 0.0768095).
```
### Confidence Interval with Covariance