

# 4.3 Bernoulli trials and the binomial distribution
## Bernoulli trials
a sequence of trials each with just two possible outcomes, “success” and “failure” and Pr(success) = ⇡, independent of the outcome of any other trial.
## Binomial distribution

Probability mass function:
```{math}
P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}
```

Cumulative distribution function:
$$
P(X \leq k) = \sum_{i=0}^{k} \binom{n}{i} p^i (1-p)^{n-i}
$$

Mean:
$$
\mu = np
$$

Variance:
$$
\sigma^2 = np(1-p)
$$

## Normal approximation
```{math}
P(a \leq X \leq b) \approx \Phi\left(\frac{b+0.5-np}{\sqrt{np(1-p)}}\right) - \Phi\left(\frac{a-0.5-np}{\sqrt{np(1-p)}}\right)
```
In this equation, $P(a \leq X \leq b)$ represents the probability of the binomial random variable X falling within the range from a to b, and $\Phi$ denotes the cumulative distribution function of the standard normal distribution.


If $ Y = Bin(m,p)$ and $ X = N(mp, mp(1-p))$ then $$ Y \stackrel{d}{\approx} X $$

$$ Pr(Y = y) \approx Pr(y - 0.5 \leq X \leq y + 0.5) $$
provided both mp and m(1-p) are $\geq$ 5

# 4.4 Link functions for binomial data
Logit link:
```{math}
:label: logit
logit(\pi) = ln(\frac{\pi}{1-\pi})
```
Probit link:
```{math}
:label: probit
probit(\pi) = \phi^{-1}(\pi)
```
Complementary log-log:
```{math}
:label: cloglog
comploglog(\pi) = ln(-ln(1-\pi))
```
log-log link:
```{math}
:label: loglog
loglog{\pi} = ln(-ln(\pi))
```

## Properties of logit function

```{math}
Prob &= \pi

Odds &= \frac{\pi}{1-\pi} = \phi

log(odds) &= logit{\pi}

ln(\phi) &= logit(\pi) = log-odds \stackrel{denote}{=} \gamma

logit(1-\pi) &= -logit(\pi)

logit(0) &= -\infty; logit(0.5) = 0, logit(1) = \infty

```
```{image} /image/probodds.png
:alt: probodds
:class: bg-primary mb-1
:width: 300px
:align: center
```

## Parameter estimation

> Uses the method of maximum likelihood (iteratively reweighted least squares or IRWLS — see Chapter 3)

> Exact properties of estimators (distributions, means, variances etc) not known but, asymptotically, estimators are normally distributed, unbiased with variance (or dispersion) matrix given by the inverse of the“Information Matrix”, see Chapter 3.
