/* cSpell:disable */
# 2.1 Heart Disease Example
> This study focused on finding the influential factors of getting(yes or no) a coronary heart diseas(chd) by fitting logistic regression (as the response is binary).

## Logistic Regression

```{admonition} When to use
Suppose the response variable Y is binary with P (Y = 1) = p and P (Y = 0) = 1 p.
Thus Y follows a Bernoulli (a special binomial) distribution. 
```


### Statistic Model
```{math}
:label: main_idea
log\frac{p_i}{1-p_i} &= \beta_0 + \beta_1x_{i1} + ... + \beta_qx_{iq}\\
\frac{p_i}{1-p_i} &= e^{\beta_0} \cdot e^{\beta_1x_{i1}} \cdot ... \cdot e^{\beta_qx_{iq}}
```
Link predictor & Link & Probability <br>
```{math}
:label: p_and_e
\eta_i &= log\frac{p_i}{1-p_i}\\
p_i &= \frac{e^{\eta_i }}{1+ e^{\eta_i }} =  \frac{1}{1+ e^{-\eta_i }}\\
e^{\eta_i} &= \frac{p_i}{1-p_i}\\
1-p_i &= \frac{1}{1+ e^{\eta_i}} =  \frac{e^{-\eta_i }}{1+ e^{-\eta_i}}
```
### Parameter Names
```{admonition} Names
Linear predictor: $$\eta_i$$
Logistic link: $$ \eta_i = log\frac{p_i}{1-p_i}$$
Log odds: $$log\frac{p_i}{1-p_i}$$
Odds: $$\frac{p_i}{1-p_i}$$
Odds ratio: $$e^{\eta}$$
Note: $\eta_i = \beta_0 + \beta_1x_{i1} = \boldsymbol{x_i}^\top \boldsymbol{\beta}$
```
### Parameter estimation
```{admonition} Main Idea
To estimate $\beta$ by the method of maximum likelihood This can be achieved withe the log-likelihood function $l(\beta)$.
```
#### Log Likelihood
The likelihood function (often simply called the likelihood) is the joint probability of the observed data viewed as a function of the parameters of a statistical model. Mathmatical proof can be seen at this 
<a href="https://www.statlect.com/fundamentals-of-statistics/logistic-model-maximum-likelihood" target="_blank">link</a>.
```{math}
:label: log-like
l(\boldsymbol{\beta} | \boldsymbol{y}, \boldsymbol{X}) &= \sum_{i=1}^{n} \left[y_i \log p_i + (1 - y_i) \log (1 - p_i)\right]\\
l(\boldsymbol{\beta} | \boldsymbol{y}, \boldsymbol{X}) &= \sum_{i=1}^{n} y_i \log \left(\frac{1}{1 + e^{-\boldsymbol{x_i}^\top \boldsymbol{\beta}}}\right) + (1 - y_i) \log \left(\frac{e^{-\boldsymbol{x_i}^\top \boldsymbol{\beta}}}{1 + e^{-\boldsymbol{x_i}^\top \boldsymbol{\beta}}}\right)
```
This transformation is done with {eq}`p_and_e`
#### Score function
first derivative of log-likelihood function
```{math}
:label: sf
u(\beta) = \frac{\partial \boldsymbol{l}(\boldsymbol{\beta} | \boldsymbol{y}, \boldsymbol{X})}{\partial \boldsymbol{\beta}} = \sum_{i=1}^{n} (\boldsymbol{x_i}^\top (\boldsymbol{y} - \boldsymbol{p}))
```

#### Hessian function
second derivative of log-likelihood function
```{math}
:label: hf
\boldsymbol{H}(\boldsymbol{\beta} | \boldsymbol{X}) = -\sum_{i=1}^{n} p_i (1-p_i) \boldsymbol{x_i} \boldsymbol{x_i}^\top
```
#### Observed information
Negative Hessian function {eq}`hf`
$$ 
\boldsymbol{I}(\boldsymbol{\beta} | \boldsymbol{X}) = -\frac{\partial^2 \boldsymbol{l}(\boldsymbol{\beta} | \boldsymbol{y}, \boldsymbol{X})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top} = \sum_{i=1}^{n} p_i (1-p_i) \boldsymbol{x_i} \boldsymbol{x_i}^\top
$$ 

#### Fisher information
Negative Expetation of Hessian function {eq}`hf`
```{math}
:label: fi
\boldsymbol{J}(\boldsymbol{\beta} | \boldsymbol{X}) = \mathbb{E} \left[ -\frac{\partial^2 \log f(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top} \right] = \mathbb{E} \left[ \sum_{i=1}^{n} p_i (1-p_i) \boldsymbol{x_i} \boldsymbol{x_i}^\top \righLEt]
```
```{tip}
For logistic regression Fisher Information is equivalent to Observed information
```
#### MLE
$$
\hat{\boldsymbol{\beta}} = \arg\max_{\boldsymbol{\beta}} \boldsymbol{l}(\boldsymbol{\beta} | \boldsymbol{y}, \boldsymbol{X})
$$
$$
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \left(\boldsymbol{J}(\boldsymbol{\beta}^{(t)})\right)^{-1} \boldsymbol{u}(\boldsymbol{\beta}^{(t)})
$$
Where J is fisher information {eq}`fi`, u is score function{eq}`sf`

---
#### How to find MLE (ChatGPT)
Sure, here's the reformatted answer with $..$ for in-text LaTeX:

To find the estimate of the maximum likelihood estimator (MLE) for a given statistical model, you need to follow these steps:

1. Write down the likelihood function for the model, which is the probability of observing the data given the model parameters.

2. Take the natural logarithm of the likelihood function to obtain the log-likelihood function.

3. Compute the partial derivatives of the log-likelihood function with respect to each model parameter.

4. Set each partial derivative to zero and solve for the corresponding parameter value(s). These values correspond to the maximum of the log-likelihood function, and therefore, they are the estimates of the MLE.

5. Check that the second derivative of the log-likelihood function evaluated at the MLE estimate(s) is negative, which ensures that the MLE estimate(s) correspond to a maximum rather than a minimum or a saddle point.

Here is an example of finding the MLE estimate for the mean parameter of a normal distribution:

1. The likelihood function for a random sample of size $n$ from a normal distribution with mean $\mu$ and variance $\sigma^2$ is given by:

$$ L(\mu, \sigma^2) = (2\pi\sigma^2)^{-n/2} \cdot \exp\left[-\frac{\sum\left((x_i - \mu)^2\right)}{2\sigma^2}\right],$$ 

where $x_i$ are the observed data points.

2. The log-likelihood function is:

$$ \ln L(\mu, \sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum\left((x_i - \mu)^2\right).$$ 

3. The partial derivative of the log-likelihood function with respect to $\mu$ is:

$$ \frac{\partial}{\partial \mu}\ln L(\mu, \sigma^2) = \frac{1}{2\sigma^2}\sum(x_i - \mu).$$ 

4. Setting the partial derivative to zero and solving for $\mu$ gives:

$$ \sum(x_i - \mu) = 0,$$ 

which implies that:

$$ \mu = \frac{1}{n}\sum x_i.$$ 

Therefore, the MLE estimate for $\mu$ is the sample mean.

5. The second derivative of the log-likelihood function with respect to $\mu$ is:

$$\frac{\partial^2}{\partial \mu^2}\ln L(\mu, \sigma^2) = -\frac{n}{2\sigma^2},$$

which is negative for all values of $\mu$ and $\sigma^2$, so the MLE estimate for $\mu$ corresponds to a maximum of the log-likelihood function.

---

## Useful Coding
### Exploratory Data Analysis
```{code}
# Read data
dim(wcgs)
head(wcgs) 
summary(wcgs)

# Plot data
# Box plot
plot(height ~ chd, wcgs); wcgs$y <- ifelse(wcgs$chd == "no",0,1)
plot(jitter(y,0.1)~jitter(height),wcgs,xlab="Height",ylab="Heart Disease",pch=".") 

# Scatter Plot
ggplot(wcgs,aes(x=height,color=chd))+geom_histogram(position="dodge",binwidth=1)
ggplot(wcgs, aes(x=cigs, color=chd)) + geom_histogram(position="dodge", binwidth=5, aes(y=..density..))
```
```{note}
From the above plots we found that there is relationship pattern between height, cigarette usage and chd. Although we want to fit a linear model, the response of linear model can't be binary. Hence we use <b>logistic regression</b> to present the response (chd) in terms of probability
```
### LR model fit
```{code}
lmod <- glm(chd ~ height + cigs, family = binomial, wcgs) 
beta <- coef(lmod); summary(lmod) 
plot(jitter(y,0.1) ~ jitter(height), wcgs, xlab="Height", ylab="Heart Disease",
pch=".")
```
> In R, adding jitter to a plot means adding a small amount of random noise to the data points to help visualize overlapping points. This can be useful when visualizing categorical or discrete data, as it can help distinguish data points that are otherwise indistinguishable due to their exact values being the same. 

Here is a [reference to the intro](intro.md). Here is a reference to [](section-label).1